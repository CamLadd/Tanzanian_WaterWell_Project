{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Summary and Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanzania, as a developing country, struggles with providing clean water to its population of over 57,000,000. There are many waterpoints already established in the country, but some are in need of repair while others have failed altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a classifier to predict the condition of a water well, using information provided in the data. This information includes:\n",
    "- Date\n",
    "- Location\n",
    "- Source\n",
    "- Funder\n",
    "- And more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is from the DrivenData.org website. It is part of the \"Pump It Up: Data Mining the Water Table\" dfetition. DrivenData decided to split the data up into two sets, the \"Training Set\" and the \"Test Set\". \n",
    "\n",
    "It is implied by the names that we are to use the training set for creating our models, and the test set to test them. For this project, we considered merging the two dataframes in order to have more data to work with, however there are 59,400 entries in the training set and therefore more than enough to make good predicitons. \n",
    "\n",
    "If our models are subpar, we may merge the tables to aquire more data points to potentially improve model efficacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# plot_confusion_matrix is a handy visual tool, added in the latest version of scikit-learn\n",
    "# if you are running an older version, comment out this line and just use confusion_matrix\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "# ignore all future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Load data into  Pandas dataframes\n",
    "status_groups = pd.read_csv('status_groups.csv')\n",
    "testset = pd.read_csv('test_set.csv')\n",
    "df = pd.read_csv('training_set.csv')\n",
    "\n",
    "\n",
    "# Let's add our target series to the dataframe!\n",
    "status_groups.drop(['id'], axis=1, inplace=True)\n",
    "df = pd.concat([df, status_groups], axis=1)\n",
    "\n",
    "# Analyze shape of dataset\n",
    "print(f'Shape of dataset: {df.shape}')\n",
    "# display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unneeded columns and deal with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create column that shows the age of the well at the time of recording\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "df['date_recorded'] = pd.to_datetime(df['date_recorded'])\n",
    "# recorded_year = [x.year for x in df.date_recorded]\n",
    "# df['well_age'] = recorded_year - df.construction_year\n",
    "# df.well_age.value_counts()\n",
    "# df['well_age'][df.well_age<0]\n",
    "# df.iloc[[10441, 8729, 13366, 23373, 27501, 32619, 33942, 39559]][['construction_year', 'date_recorded', 'status_group']]\n",
    "\n",
    "\n",
    "# Interestingly enough, there are some negative values for the age of wells\n",
    "# This indicates that the well was PLANNED on being built at the time of recording, but had not yet been recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORATORY! We are analyzing the columns in order to remove redundancies\n",
    "# **Uncomment if viewing the unique values is desired**\n",
    "\n",
    "# df.region.unique()\n",
    "# df.scheme_management\n",
    "\n",
    "# display(df['extraction_type_class'].unique())\n",
    "# display(df['extraction_type'].unique())\n",
    "# display(df['extraction_type_group'].unique())\n",
    "\n",
    "# display(df['source_class'].unique())\n",
    "# display(df['source'].unique())\n",
    "# display(df['source_type'].unique())\n",
    "\n",
    "# display(df.waterpoint_type.unique())\n",
    "# display(df.waterpoint_type_group.unique())\n",
    "\n",
    "# display(df.water_quality.unique())\n",
    "# display(df.quality_group.unique())\n",
    "\n",
    "# display(df.management.unique())\n",
    "# display(df.management_group.unique())\n",
    "\n",
    "# display(df.payment.unique())\n",
    "# display(df.payment_type.unique())\n",
    "\n",
    "# display(df.quantity.unique())\n",
    "# display(df.quantity_group.unique())\n",
    "\n",
    "df.isna().sum()[df.isna().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode the target variable\n",
    "status_labels = {'status_group':{'non functional': 0, 'functional': 1, 'functional needs repair': 2}}\n",
    "df = df.replace(status_labels)\n",
    "df.status_group.value_counts()\n",
    "\n",
    "\n",
    "# Drop redundant and unneeded columns\n",
    "to_drop = ['scheme_name', 'recorded_by', 'wpt_name', 'extraction_type', 'extraction_type_group',\n",
    "           'region_code', 'district_code', 'lga', 'ward', 'public_meeting', 'date_recorded', \n",
    "           'source', 'source_class', 'waterpoint_type', 'water_quality', 'management_group', \n",
    "           'payment', 'quantity_group','subvillage', 'num_private', 'scheme_management']\n",
    "\n",
    "# I am keeping latitude and longitude\n",
    "\n",
    "# Deal with missing values\n",
    "df.drop(to_drop, axis=1, inplace=True)\n",
    "df.permit.fillna(False, inplace=True)\n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Set 'id' as the index of the dataframe\n",
    "df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Labeling Column Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funder\n",
    "df.funder.replace(to_replace='0', value='unknown', inplace=True)\n",
    "df.funder.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other = list(df.funder[df['funder'].map(df['funder'].value_counts()) < 484].values)\r\n",
    "other\r\n",
    "df['funder'].replace(other, 'other', inplace=True)\r\n",
    "\r\n",
    "df.funder.replace(to_replace='0', value='Unknown', inplace=True)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer\n",
    "\n",
    "other = list(df.installer[df['installer'].map(df['installer'].value_counts()) < 392].values)\n",
    "other\n",
    "df['installer'].replace(other, 'other', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permit\n",
    "\n",
    "df.permit.replace({True:1, False:0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population\n",
    "    \n",
    "def population(obs):\n",
    "    s=''\n",
    "    x=obs['population']\n",
    "    if(0<x<=100):\n",
    "        s='Less than 100'\n",
    "    elif(100<x<=200):\n",
    "        s='Between 100 and 200'\n",
    "    elif(200<x<=300):\n",
    "        s='Between 200 and 300'\n",
    "    elif(300<x<=400):\n",
    "        s='between 300 and 400'\n",
    "    elif(400<x<=500):\n",
    "        s='between 400 and 500'\n",
    "    elif(500<x):\n",
    "        s='Over 500'\n",
    "    elif(x==0):\n",
    "        s='No population'\n",
    "    return s\n",
    "df['population']=df.apply(population,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Well_age\n",
    "\n",
    "# # Drop all items that have a value less than 0 (very few)\n",
    "# df.drop(df[df['well_age'] < 0].index, inplace = True)\n",
    "\n",
    "# # Bin\n",
    "# conditions = [df.well_age==0, (df.well_age>0)&(df.well_age<=4), (df.well_age>4)&(df.well_age<=12), (df.well_age>12)&(df.well_age<=25), \n",
    "#               (df.well_age>25)&(df.well_age<=48), df.well_age>48]\n",
    "# choices = ['new', '0-4 years', '4-12 years', '12-25 years', '25-48 years', 'more than 48 years']\n",
    "# df['well_age'] = np.select(conditions, choices)\n",
    "\n",
    "conditions = [df['construction_year']==0, (df['construction_year']>=1960)&(df['construction_year']<=1970), (df['construction_year']>1970)&(df['construction_year']<=1980),\n",
    "             (df['construction_year']>1980)&(df['construction_year']<=1990), (df['construction_year']>1990)&(df['construction_year']<=2000),\n",
    "             (df['construction_year']>2000)&(df['construction_year']<=2010), df['construction_year']>2010]\n",
    "choices = ['no_construction_year', '1960_1970', '1971_1980', '1981_1990', '1991_2000', '2001_2010', '2011_over']\n",
    "df['construction_year'] = np.select(conditions, choices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount_tsh\n",
    "# Bin\n",
    "conditions = [df.amount_tsh==0,(df.amount_tsh>0)&(df.amount_tsh<=10),(df.amount_tsh>10)&(df.amount_tsh<=100), (df.amount_tsh>100)&(df.amount_tsh<=1000),\n",
    "             (df.amount_tsh>1000)&(df.amount_tsh<=2000), (df.amount_tsh>2000)&(df.amount_tsh<=10000), (df.amount_tsh>10000)&(df.amount_tsh<=100000),\n",
    "             df.amount_tsh>100000]\n",
    "choices = ['zero', '1 to 10', '11 to 100', '101 to 1k', '1k to 2k', '2k to 10k', '10k to 100k', 'greater than 100k']\n",
    "df['amount_tsh'] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gps_height = pd.qcut(df.gps_height, 8, duplicates='drop', \n",
    "        labels=['-90m - sea level', 'sea level to 46m', '46m to 393m', '393m to 1017m', '1017m to 1316m', '1316m to 1586.75m', '1586.75m to 2770m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.funder.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.installer.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[df['funder']== 'Government Of Tanzania']\r\n",
    "df2 = df.loc[df['funder']== 'Tasaf']              \r\n",
    "df3 = df.loc[df['funder']== 'Danida'] \r\n",
    "df4 = df.loc[df['funder']== 'Hesawa'] \r\n",
    "df5 = df.loc[df['funder']== 'Rwssp'] \r\n",
    "df6 = df.loc[df['funder']== 'World Bank'] \r\n",
    "df7 = df.loc[df['funder']== 'Kkkt'] \r\n",
    "df8 = df.loc[df['funder']== 'World Vision']\r\n",
    "df9 = df.loc[df['funder']== 'Unicef'] \r\n",
    "df10 = df.loc[df['funder']== 'unknown'] \r\n",
    "df11 = df.loc[df['funder']== 'District Council'] \r\n",
    "df12 = df.loc[df['funder']== 'Dhv'] \r\n",
    "df13 = df.loc[df['funder']== 'Private Individual'] \r\n",
    "df14 = df.loc[df['funder']== 'Dwsp'] \r\n",
    "df15 = df.loc[df['funder']== 'Norad'] \r\n",
    "df16 = df.loc[df['funder']== 'Germany Republi']\r\n",
    "df17 = df.loc[df['funder']== 'Tcrs']\r\n",
    "df18 = df.loc[df['funder']== 'Ministry Of Water']\r\n",
    "df19 = df.loc[df['funder']== 'Water']\r\n",
    "df20 = df.loc[df['funder']== 'Dwe']\r\n",
    "\r\n",
    "top_20 = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,\r\n",
    "                    df13,df14,df15,df16,df17,df18,df19,df20], ignore_index=True)\r\n",
    "\r\n",
    "fig, ax = plt.subplots(figsize=(20,20))\r\n",
    "sns.countplot(x='funder', hue=\"status_group\", data=top_20)\r\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25,25))\r\n",
    "sns.countplot(x='funder', hue='status_group', data=top_20)\r\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25,25))\n",
    "sns.countplot(x='funder', hue='status_group', data=df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\r\n",
    "sns.scatterplot(x='latitude', y='longitude', hue='status_group',data=df)\r\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.countplot(x='waterpoint_type_group', hue='status_group', data=df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.countplot(x='source_type', hue='status_group', data=df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.countplot(x='quality_group', hue='status_group', data=df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.countplot(x='payment_type', hue='status_group', data=df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.countplot(x='construction_year', hue='status_group', data=df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.countplot(x='population', hue='status_group', data=df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.countplot(x='quantity', hue='status_group', data=df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sns.countplot(x='region', hue='status_group', data=df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.countplot(x='management', hue='status_group', data=df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.countplot(x='amount_tsh', hue='status_group', data=df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sns.countplot(x='basin', hue='status_group', data=df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.countplot(x='permit', hue='status_group', data=df)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.status_group.value_counts().plot(kind='bar', color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define X and y and Create a dummied Feature df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify features and target\r\n",
    "features = df.drop('status_group', axis=1)\r\n",
    "target = df.status_group\r\n",
    "\r\n",
    "# Dummy the features\r\n",
    "features_dummied = pd.get_dummies(features, drop_first=True)\r\n",
    "features_dummied.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_dummied, target, test_size=0.3, random_state=123)\r\n",
    "\r\n",
    "# Fit and cross-validate Decision Tree\r\n",
    "tree = DecisionTreeClassifier(random_state=123)\r\n",
    "cross_val_score(tree, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use recursive feature elimination to trim dataset columns\r\n",
    "# from sklearn.feature_selection import RFE\r\n",
    "# rfe = RFE(tree, 20)\r\n",
    "# rfe = rfe.fit(X_train, y_train)\r\n",
    "\r\n",
    "# rfe_columns = pd.Series(X_train.columns)\r\n",
    "# rfe_code = pd.Series(rfe.support_)\r\n",
    "# columns_to_keep = pd.concat([rfe_columns, rfe_code], axis=1)\r\n",
    "\r\n",
    "# rfe_X_train = X_train[list(columns_to_keep[columns_to_keep[1]==True][0])]\r\n",
    "# rfe_X_test = X_test[list(columns_to_keep[columns_to_keep[1]==True][0])]\r\n",
    "\r\n",
    "# Cross validate again\r\n",
    "# cross_val_score(tree, rfe_X_train, y_train, cv=10)\r\n",
    "\r\n",
    "# print(rfe.support_)\r\n",
    "# print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building  Random Forest Classifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.metrics import f1_score, accuracy_score\r\n",
    "rfc = RandomForestClassifier(criterion = 'entropy', random_state = 42)\r\n",
    "rfc.fit(X_train, y_train)\r\n",
    "\r\n",
    "# Evaluating on Training set\r\n",
    "rfc_pred_train = rfc.predict(X_train)\r\n",
    "print('Training Set Evaluation F1-Score=>', f1_score(y_train,rfc_pred_train, average=None))\r\n",
    "print('Training Set Evaluation Accuracy-Score=>', accuracy_score(y_train,rfc_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data sets\r\n",
    "# from sklearn.preprocessing import StandardScaler\r\n",
    "# scaler = StandardScaler()\r\n",
    "# scaler.fit_transform(features_dummied, target)\r\n",
    "\r\n",
    "# Split the data into training and test sets\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_dummied, target, test_size=0.3, random_state=123)\r\n",
    "\r\n",
    "# Fit and cross-validate Logistic Regression\r\n",
    "lr = LogisticRegression(max_iter=10000, random_state=123)\r\n",
    "cross_val_score(lr, X_train, y_train)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# # Use recursive feature elimination to create a trimmed dataset that could potentially improve score\r\n",
    "# from sklearn.feature_selection import RFE\r\n",
    "# rfe = RFE(tree, 20)\r\n",
    "# rfe = rfe.fit(X_train, y_train)\r\n",
    "\r\n",
    "# rfe_columns = pd.Series(X_train.columns)\r\n",
    "# rfe_code = pd.Series(rfe.support_)\r\n",
    "# columns_to_keep = pd.concat([rfe_columns, rfe_code], axis=1)\r\n",
    "\r\n",
    "# rfe_X_train = X_train[list(columns_to_keep[columns_to_keep[1]==True][0])]\r\n",
    "# rfe_X_test = X_test[list(columns_to_keep[columns_to_keep[1]==True][0])]\r\n",
    "\r\n",
    "# Cross validate again\r\n",
    "# cross_val_score(lr, rfe_X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.64      0.71      6536\n",
      "           1       0.73      0.90      0.80      9032\n",
      "           2       0.56      0.11      0.18      1140\n",
      "\n",
      "    accuracy                           0.74     16708\n",
      "   macro avg       0.69      0.55      0.56     16708\n",
      "weighted avg       0.74      0.74      0.72     16708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display classification report\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "lr.fit(X_train, y_train)\r\n",
    "y_preds = lr.predict(X_test)\r\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-0073914357fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlogit_roc_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \"\"\"\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    278\u001b[0m             \u001b[1;32mclass\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \"\"\"\n\u001b[1;32m--> 280\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1019\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "from sklearn.metrics import roc_curve\r\n",
    "logit_roc_auc = roc_auc_score(y_test, lr.predict(X_test), multi_class='ovr')\r\n",
    "fpr, tpr, thresholds = roc_curve(y_test, lr.predict_proba(X_test)[:,1])\r\n",
    "plt.figure()\r\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\r\n",
    "plt.plot([0, 1], [0, 1],'r--')\r\n",
    "plt.xlim([0.0, 1.0])\r\n",
    "plt.ylim([0.0, 1.05])\r\n",
    "plt.xlabel('False Positive Rate')\r\n",
    "plt.ylabel('True Positive Rate')\r\n",
    "plt.title('Receiver operating characteristic')\r\n",
    "plt.legend(loc=\"lower right\")\r\n",
    "plt.savefig('Log_ROC')\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONE VS ALL IN SKLEARN\r\n",
    "ROC CURVE"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02a9839076075b91ad0ca4473d7f7ceb754ea3983ec7c01053cc622904b7a0ac"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('learn-env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}